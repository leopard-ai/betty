


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <!-- Google Tag Manager -->
  <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
  new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
  j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
  'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
  })(window,document,'script','dataLayer','GTM-PQBQ3CV');
  </script>
  <!-- End Google Tag Manager -->

  <meta charset="utf-8">
  
  <meta name="google-site-verification" content="okUst94cAlWSsUsGZTB4xSS4UKTtRV8Nu5XZ9pdd3Aw" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>betty.problems.problem &mdash; Betty</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/custom.css" type="text/css" />
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
  <!-- Google Analytics -->
  
  <!-- End Google Analytics -->
  

  
  <script src="../../../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/UCity/UCity-Light.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/UCity/UCity-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/UCity/UCity-Semibold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/Inconsolata/Inconsolata.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <script defer src="https://use.fontawesome.com/releases/v6.1.1/js/all.js" integrity="sha384-xBXmu0dk1bEoiwd71wOonQLyH+VpgR1XcDH3rtxrLww5ajNTuMvBdL5SOiFZnNdp" crossorigin="anonymous"></script>
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://leopard-ai.github.io/betty" aria-label="PyTorch Lightning">
      <!--  <img class="call-to-action-img" src="../../../_static/images/logo-lightning-icon.png"/> -->
      </a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://leopard-ai.github.io/betty">Docs</a>
          </li>

          <li>
            <a href="https://github.com/leopard-ai/betty">GitHub</a>
          </li>

        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            

            
              
              
            

            


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

            
          </div>

          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Get Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../quickstart/installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../quickstart/concept.html">Major Concepts</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Tutorials</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../tutorial/basic/basic.html">Basic</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tutorial/intermediate/intermediate.html">Intermediate</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API Docs</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../betty/betty.engine.html">betty.engine</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../betty/betty.problems.html">betty.problems</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../betty/betty.hypergradient.html">betty.hypergradient</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../betty/betty.optim.html">betty.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../betty/betty.logging.html">betty.logging</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Examples</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../examples/data_reweighting.html">Data Reweighting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../examples/maml.html">Model-Agnostic Meta-Learning (MAML)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../examples/neural_architecture_search.html">Neural Architecture Search</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../../../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="../../index.html">Module code</a> &gt;</li>
        
      <li>betty.problems.problem</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <h1>Source code for betty.problems.problem</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright Sang Keun Choe</span>
<span class="c1">#</span>
<span class="c1"># This source code is licensed under the MIT license found in the</span>
<span class="c1"># LICENSE file in the root directory of this source tree.</span>

<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">abc</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.distributed</span> <span class="k">as</span> <span class="nn">dist</span>

<span class="kn">from</span> <span class="nn">betty.patch.data_loader</span> <span class="kn">import</span> <span class="n">get_distributed_data_loader</span>
<span class="kn">from</span> <span class="nn">betty.patch.optimizer</span> <span class="kn">import</span> <span class="n">patch_optimizer</span>
<span class="kn">from</span> <span class="nn">betty.patch.scheduler</span> <span class="kn">import</span> <span class="n">patch_scheduler</span>
<span class="kn">from</span> <span class="nn">betty.configs</span> <span class="kn">import</span> <span class="n">Config</span>
<span class="kn">from</span> <span class="nn">betty.hypergradient</span> <span class="kn">import</span> <span class="n">get_grads</span>
<span class="kn">from</span> <span class="nn">betty.utils</span> <span class="kn">import</span> <span class="n">convert_tensor</span><span class="p">,</span> <span class="n">log_from_loss_dict</span>


<div class="viewcode-block" id="Problem"><a class="viewcode-back" href="../../../betty/betty.problems.html#betty.problems.problem.Problem">[docs]</a><span class="k">class</span> <span class="nc">Problem</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This is the base class for an optimization problem in multilevel optimization.</span>
<span class="sd">    Specifically, each problem is defined by the parameter (or module), the sets of the upper</span>
<span class="sd">    and lower constraining problems, the dataset, the loss function, the optimizer, and other</span>
<span class="sd">    optimization configurations (e.g. best-response Jacobian calculation algorithm, number of</span>
<span class="sd">    unrolling steps, etc.).</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">name</span><span class="p">,</span>
        <span class="n">config</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">module</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">optimizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">scheduler</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">train_data_loader</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">extra_config</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="c1"># basic configurations</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_name</span> <span class="o">=</span> <span class="n">name</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_config</span> <span class="o">=</span> <span class="n">config</span> <span class="k">if</span> <span class="n">config</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">Config</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cfg</span> <span class="o">=</span> <span class="n">extra_config</span>

        <span class="c1"># device</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1"># distributed</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_strategy</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">accelerator</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_distributed</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_backend</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_world_size</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_rank</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_local_rank</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1"># computation graph depedency</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_parents</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_children</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_paths</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="c1"># data loader</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">train_data_loader</span> <span class="o">=</span> <span class="n">train_data_loader</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">train_data_iterator</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cur_batch</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">epoch_counter</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1"># module</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">module</span> <span class="o">=</span> <span class="n">module</span>

        <span class="c1"># optimizer &amp; lr scheduler</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scheduler</span> <span class="o">=</span> <span class="n">scheduler</span>

        <span class="c1"># environment</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">env</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1"># fp16 scaler</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_fp16</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">fp16</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scaler</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fp16</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">initial_dynamic_scale</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">initial_dynamic_scale</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">scale_factor</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">scale_factor</span>

        <span class="c1"># gradient accumulation</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gas</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">gradient_accumulation</span>

        <span class="c1"># gradient clipping</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gradient_clipping</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">gradient_clipping</span>

        <span class="c1"># warmup</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">warmup_steps</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">warmup_steps</span>

        <span class="c1"># logger</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log_step</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">log_step</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log_local_step</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">log_local_step</span>

        <span class="c1"># step counter</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_count</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_global_step</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="c1"># misc</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_leaf</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_first_order</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_retain_graph</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">retain_graph</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_allow_unused</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">allow_unused</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_unroll_steps</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">unroll_steps</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_roll_back</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_inner_loop_start</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_training</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ready</span> <span class="o">=</span> <span class="kc">None</span>

<div class="viewcode-block" id="Problem.initialize"><a class="viewcode-back" href="../../../betty/betty.problems.html#betty.problems.problem.Problem.initialize">[docs]</a>    <span class="k">def</span> <span class="nf">initialize</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        ``initialize`` patches/sets up module, optimizer, data loader, etc. after compiling a</span>
<span class="sd">        user-provided configuration (e.g., fp16 training, iterative differentiation)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># initialize update ready to False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ready</span> <span class="o">=</span> <span class="p">[</span><span class="kc">False</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_children</span><span class="p">))]</span>

        <span class="c1"># compile parents configurations</span>
        <span class="n">first_order</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">problem</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_parents</span><span class="p">:</span>
            <span class="n">parent_config</span> <span class="o">=</span> <span class="n">problem</span><span class="o">.</span><span class="n">config</span>
            <span class="n">first_order</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">parent_config</span><span class="o">.</span><span class="n">first_order</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_first_order</span> <span class="o">=</span> <span class="nb">all</span><span class="p">(</span><span class="n">first_order</span><span class="p">)</span>

        <span class="c1"># set inner_loop_start to True</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_inner_loop_start</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="c1"># set up data loader</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_implemented</span><span class="p">(</span><span class="s2">&quot;configure_train_data_loader&quot;</span><span class="p">):</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_data_loader</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">train_data_loader</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">configure_train_data_loader</span><span class="p">()</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_data_loader</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">train_data_loader</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">train_data_loader</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">train_data_loader</span><span class="p">,)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_implemented</span><span class="p">(</span><span class="s2">&quot;get_batch&quot;</span><span class="p">)</span>

        <span class="c1"># set up module</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_implemented</span><span class="p">(</span><span class="s2">&quot;configure_module&quot;</span><span class="p">):</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">module</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">module</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">configure_module</span><span class="p">()</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">module</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;Module must be specified!&quot;</span>

        <span class="c1"># set up optimizer</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_implemented</span><span class="p">(</span><span class="s2">&quot;configure_optimizer&quot;</span><span class="p">):</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">configure_optimizer</span><span class="p">()</span>

        <span class="c1"># set up lr scheduler</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_implemented</span><span class="p">(</span><span class="s2">&quot;configure_scheduler&quot;</span><span class="p">):</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">scheduler</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">scheduler</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">configure_scheduler</span><span class="p">()</span>

        <span class="c1"># set up fp16 training</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_default_fp16</span><span class="p">():</span>
            <span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span>
            <span class="n">scaler_cls</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">amp</span><span class="o">.</span><span class="n">GradScaler</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_strategy</span> <span class="o">==</span> <span class="s2">&quot;fsdp&quot;</span><span class="p">:</span>
                <span class="kn">from</span> <span class="nn">torch.distributed.fsdp.sharded_grad_scaler</span> <span class="kn">import</span> <span class="n">ShardedGradScaler</span>

                <span class="n">scaler_cls</span> <span class="o">=</span> <span class="n">ShardedGradScaler</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">scaler</span> <span class="o">=</span> <span class="n">scaler_cls</span><span class="p">(</span>
                <span class="n">init_scale</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">initial_dynamic_scale</span><span class="p">,</span> <span class="n">growth_factor</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">scale_factor</span>
            <span class="p">)</span>

        <span class="c1"># patch module, optimizer, data loader, and scheduler</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">patch_everything</span><span class="p">()</span>

        <span class="c1"># make train_data_loader as iterator</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_data_loader</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">train_data_iterator</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">epoch_counter</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">train_data_loader</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_data_loader</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">train_data_iterator</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">train_data_loader</span><span class="p">))</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">epoch_counter</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

        <span class="c1"># Logging INFO</span>
        <span class="n">path_str</span> <span class="o">=</span> <span class="p">[[</span><span class="n">node</span><span class="o">.</span><span class="n">name</span> <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">path</span><span class="p">]</span> <span class="k">for</span> <span class="n">path</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_paths</span><span class="p">]</span>
        <span class="n">children_str</span> <span class="o">=</span> <span class="p">[</span><span class="n">node</span><span class="o">.</span><span class="n">name</span> <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_children</span><span class="p">]</span>
        <span class="n">parents_str</span> <span class="o">=</span> <span class="p">[</span><span class="n">node</span><span class="o">.</span><span class="n">name</span> <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_parents</span><span class="p">]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_rank_zero</span><span class="p">():</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;*** Problem Information ***&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Name: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Uppers: </span><span class="si">{</span><span class="n">parents_str</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Lowers: </span><span class="si">{</span><span class="n">children_str</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Paths: </span><span class="si">{</span><span class="n">path_str</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span></div>

<div class="viewcode-block" id="Problem.patch_everything"><a class="viewcode-back" href="../../../betty/betty.problems.html#betty.problems.problem.Problem.patch_everything">[docs]</a>    <span class="k">def</span> <span class="nf">patch_everything</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        We patch module, optimizer, data loader, and lr scheduler for device placement,</span>
<span class="sd">        distributed training, zero optimizer, fsdp, etc.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">patch_module</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">patch_optimizer</span><span class="p">()</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">scheduler</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">patch_scheduler</span><span class="p">()</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_data_loader</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">train_data_loader</span> <span class="o">=</span> <span class="p">[</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">patch_data_loader</span><span class="p">(</span><span class="n">data_loader</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">data_loader</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_data_loader</span>
            <span class="p">]</span></div>

<div class="viewcode-block" id="Problem.patch_module"><a class="viewcode-back" href="../../../betty/betty.problems.html#betty.problems.problem.Problem.patch_module">[docs]</a>    <span class="k">def</span> <span class="nf">patch_module</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Patch module given the systems configuration (e.g., DDP, FSDP)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_strategy</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;distributed&quot;</span><span class="p">,</span> <span class="s2">&quot;zero&quot;</span><span class="p">]:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">synchronize_params</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">module</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">parallel</span><span class="o">.</span><span class="n">DistributedDataParallel</span><span class="p">(</span>
                <span class="n">module</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="p">,</span>
                <span class="n">gradient_as_bucket_view</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">_strategy</span> <span class="o">==</span> <span class="s2">&quot;fsdp&quot;</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_rank_zero</span><span class="p">():</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;FSDP requires PyTorch version &gt;= 1.12&quot;</span><span class="p">)</span>
            <span class="kn">from</span> <span class="nn">torch.distributed.fsdp</span> <span class="kn">import</span> <span class="n">FullyShardedDataParallel</span> <span class="k">as</span> <span class="n">FSDP</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">synchronize_params</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">module</span> <span class="o">=</span> <span class="n">FSDP</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="p">,</span> <span class="n">device_id</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">_strategy</span> <span class="o">==</span> <span class="s2">&quot;accelerate&quot;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">module</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">accelerator</span><span class="o">.</span><span class="n">prepare</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="p">)</span></div>

<div class="viewcode-block" id="Problem.patch_optimizer"><a class="viewcode-back" href="../../../betty/betty.problems.html#betty.problems.problem.Problem.patch_optimizer">[docs]</a>    <span class="k">def</span> <span class="nf">patch_optimizer</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Patch optimizer given the systems configuration (e.g., DDP, FSDP)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">params</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainable_parameters</span><span class="p">()</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_implemented</span><span class="p">(</span><span class="s2">&quot;param_groups&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_strategy</span> <span class="o">!=</span> <span class="s2">&quot;fsdp&quot;</span><span class="p">:</span>
            <span class="n">params</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">()</span>
        <span class="n">is_zero</span> <span class="o">=</span> <span class="kc">True</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_strategy</span> <span class="o">==</span> <span class="s2">&quot;zero&quot;</span> <span class="k">else</span> <span class="kc">False</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_strategy</span> <span class="o">==</span> <span class="s2">&quot;accelerate&quot;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">accelerator</span><span class="o">.</span><span class="n">prepare</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">patch_optimizer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">is_zero</span><span class="p">)</span></div>

<div class="viewcode-block" id="Problem.patch_scheduler"><a class="viewcode-back" href="../../../betty/betty.problems.html#betty.problems.problem.Problem.patch_scheduler">[docs]</a>    <span class="k">def</span> <span class="nf">patch_scheduler</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Patch scheduler given the systems configuration (e.g., DDP, FSDP)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scheduler</span> <span class="o">=</span> <span class="n">patch_scheduler</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">scheduler</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_strategy</span> <span class="o">==</span> <span class="s2">&quot;accelerate&quot;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">scheduler</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">accelerator</span><span class="o">.</span><span class="n">prepare</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">scheduler</span><span class="p">)</span></div>

<div class="viewcode-block" id="Problem.patch_data_loader"><a class="viewcode-back" href="../../../betty/betty.problems.html#betty.problems.problem.Problem.patch_data_loader">[docs]</a>    <span class="k">def</span> <span class="nf">patch_data_loader</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loader</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Patch data loader given the systems configuration (e.g., DDP, FSDP)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_strategy</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;distributed&quot;</span><span class="p">,</span> <span class="s2">&quot;zero&quot;</span><span class="p">,</span> <span class="s2">&quot;fsdp&quot;</span><span class="p">]:</span>
            <span class="n">patched_loader</span> <span class="o">=</span> <span class="n">get_distributed_data_loader</span><span class="p">(</span>
                <span class="n">loader</span><span class="p">,</span> <span class="n">world_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_world_size</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_rank</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">_strategy</span> <span class="o">==</span> <span class="s2">&quot;accelerate&quot;</span><span class="p">:</span>
            <span class="n">patched_loader</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">accelerator</span><span class="o">.</span><span class="n">prepare</span><span class="p">(</span><span class="n">loader</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">patched_loader</span> <span class="o">=</span> <span class="n">loader</span>

        <span class="k">return</span> <span class="n">patched_loader</span></div>

<div class="viewcode-block" id="Problem.set_module"><a class="viewcode-back" href="../../../betty/betty.problems.html#betty.problems.problem.Problem.set_module">[docs]</a>    <span class="k">def</span> <span class="nf">set_module</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">module</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Set new module for the current Problem class.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">module</span> <span class="o">=</span> <span class="n">module</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">patch_module</span><span class="p">()</span></div>

<div class="viewcode-block" id="Problem.set_optimizer"><a class="viewcode-back" href="../../../betty/betty.problems.html#betty.problems.problem.Problem.set_optimizer">[docs]</a>    <span class="k">def</span> <span class="nf">set_optimizer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Set new optimizer for the current Problem class.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">patch_optimizer</span><span class="p">()</span></div>

<div class="viewcode-block" id="Problem.set_scheduler"><a class="viewcode-back" href="../../../betty/betty.problems.html#betty.problems.problem.Problem.set_scheduler">[docs]</a>    <span class="k">def</span> <span class="nf">set_scheduler</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">scheduler</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Set new scheduler for the current Problem class.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scheduler</span> <span class="o">=</span> <span class="n">scheduler</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">patch_scheduler</span><span class="p">()</span></div>

<div class="viewcode-block" id="Problem.set_train_data_loader"><a class="viewcode-back" href="../../../betty/betty.problems.html#betty.problems.problem.Problem.set_train_data_loader">[docs]</a>    <span class="k">def</span> <span class="nf">set_train_data_loader</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loader</span><span class="p">,</span> <span class="n">idx</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Set new data loader for the current Problem class.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">train_data_loader</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">patch_data_loader</span><span class="p">(</span><span class="n">loader</span><span class="p">)</span></div>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

<div class="viewcode-block" id="Problem.forward"><a class="viewcode-back" href="../../../betty/betty.problems.html#betty.problems.problem.Problem.forward">[docs]</a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Users define how forward (or call) function is defined for the problem here.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></div>

<div class="viewcode-block" id="Problem.training_step"><a class="viewcode-back" href="../../../betty/betty.problems.html#betty.problems.problem.Problem.training_step">[docs]</a>    <span class="nd">@abc</span><span class="o">.</span><span class="n">abstractmethod</span>
    <span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Users define the loss function of the problem here.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span></div>

<div class="viewcode-block" id="Problem.training_step_exec"><a class="viewcode-back" href="../../../betty/betty.problems.html#betty.problems.problem.Problem.training_step_exec">[docs]</a>    <span class="k">def</span> <span class="nf">training_step_exec</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_default_fp16</span><span class="p">():</span>
            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">amp</span><span class="o">.</span><span class="n">autocast</span><span class="p">():</span>
                <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_step</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_step</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span></div>

<div class="viewcode-block" id="Problem.one_step_descent"><a class="viewcode-back" href="../../../betty/betty.problems.html#betty.problems.problem.Problem.one_step_descent">[docs]</a>    <span class="k">def</span> <span class="nf">one_step_descent</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="c1"># load data</span>
        <span class="k">if</span> <span class="n">batch</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">cur_batch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_batch</span><span class="p">()</span>
            <span class="n">batch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cur_batch</span>

        <span class="c1"># calculate loss</span>
        <span class="n">loss</span><span class="p">,</span> <span class="n">loss_dict</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_loss</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>

        <span class="c1"># calculate gradient (a.k.a backward)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span>
            <span class="n">loss</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span>
            <span class="n">params</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">trainable_parameters</span><span class="p">(),</span>
            <span class="n">paths</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_paths</span><span class="p">,</span>
            <span class="n">create_graph</span><span class="o">=</span><span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_first_order</span><span class="p">,</span>
            <span class="n">retain_graph</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_retain_graph</span><span class="p">,</span>
            <span class="n">allow_unused</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_allow_unused</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_implemented</span><span class="p">(</span><span class="s2">&quot;grad_callback&quot;</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">grad_callback</span><span class="p">()</span>

        <span class="c1"># calculate parameter update</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_count</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">gas</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">optimizer_step</span><span class="p">()</span>

            <span class="c1"># param callback (e.g., parameter clipping)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_implemented</span><span class="p">(</span><span class="s2">&quot;param_callback&quot;</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">param_callback</span><span class="p">()</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_strategy</span> <span class="o">!=</span> <span class="s2">&quot;default&quot;</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_count</span> <span class="o">%</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gas</span> <span class="o">*</span> <span class="mi">20</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">synchronize_params</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">trainable_parameters</span><span class="p">())</span>

            <span class="c1"># zero-out grad</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">loss_dict</span></div>

<div class="viewcode-block" id="Problem.step_normal"><a class="viewcode-back" href="../../../betty/betty.problems.html#betty.problems.problem.Problem.step_normal">[docs]</a>    <span class="k">def</span> <span class="nf">step_normal</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">global_step</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">check_ready</span><span class="p">():</span>
            <span class="c1"># loop start</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_inner_loop_start</span><span class="p">:</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_implemented</span><span class="p">(</span><span class="s2">&quot;on_inner_loop_start&quot;</span><span class="p">):</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">on_inner_loop_start</span><span class="p">()</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_inner_loop_start</span> <span class="o">=</span> <span class="kc">False</span>

                <span class="c1"># copy current parameters, buffers, optimizer states</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_roll_back</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">cache_states</span><span class="p">()</span>

            <span class="c1"># increase count (local step)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_training</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_count</span> <span class="o">+=</span> <span class="mi">1</span>

            <span class="c1"># one step grdient descent</span>
            <span class="n">loss_dict</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">one_step_descent</span><span class="p">()</span>

            <span class="c1"># lr scheduler step</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">scheduler</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_roll_back</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

            <span class="c1"># logging</span>
            <span class="k">if</span> <span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">log_step</span> <span class="o">&gt;</span> <span class="mi">0</span>
                <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_count</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">log_step</span> <span class="o">==</span> <span class="mi">0</span>
                <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_rank_zero</span><span class="p">()</span>
            <span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">loss_dict</span><span class="p">,</span> <span class="n">global_step</span><span class="p">)</span>

            <span class="c1"># call parent step_normal after unrolling</span>
            <span class="k">if</span> <span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_training</span>
                <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_count</span> <span class="o">%</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_unroll_steps</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">gas</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span>
                <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_count</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">warmup_steps</span>
            <span class="p">):</span>
                <span class="k">for</span> <span class="n">problem</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_parents</span><span class="p">:</span>
                    <span class="n">idx</span> <span class="o">=</span> <span class="n">problem</span><span class="o">.</span><span class="n">children</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
                    <span class="n">problem</span><span class="o">.</span><span class="n">ready</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
                    <span class="n">problem</span><span class="o">.</span><span class="n">step_normal</span><span class="p">(</span><span class="n">global_step</span><span class="o">=</span><span class="n">global_step</span><span class="p">)</span>

                <span class="bp">self</span><span class="o">.</span><span class="n">_inner_loop_start</span> <span class="o">=</span> <span class="kc">True</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">ready</span> <span class="o">=</span> <span class="p">[</span><span class="kc">False</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_children</span><span class="p">))]</span></div>

<div class="viewcode-block" id="Problem.step_after_roll_back"><a class="viewcode-back" href="../../../betty/betty.problems.html#betty.problems.problem.Problem.step_after_roll_back">[docs]</a>    <span class="k">def</span> <span class="nf">step_after_roll_back</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">check_ready</span><span class="p">()</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_training</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_roll_back</span><span class="p">:</span>
                <span class="c1"># recover from cached states</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">recover_states</span><span class="p">()</span>

                <span class="c1"># one step gradient step</span>
                <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">one_step_descent</span><span class="p">(</span><span class="n">batch</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">cur_batch</span><span class="p">)</span>

                <span class="c1"># lr scheduler</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">scheduler</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

                <span class="c1"># call parent step_after_roll_back</span>
                <span class="k">for</span> <span class="n">problem</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_parents</span><span class="p">:</span>
                    <span class="n">idx</span> <span class="o">=</span> <span class="n">problem</span><span class="o">.</span><span class="n">children</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
                    <span class="n">problem</span><span class="o">.</span><span class="n">ready</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
                    <span class="n">problem</span><span class="o">.</span><span class="n">step_after_roll_back</span><span class="p">()</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">ready</span> <span class="o">=</span> <span class="p">[</span><span class="kc">False</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_children</span><span class="p">))]</span></div>

<div class="viewcode-block" id="Problem.step"><a class="viewcode-back" href="../../../betty/betty.problems.html#betty.problems.problem.Problem.step">[docs]</a>    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">global_step</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        ``step`` method abstracts a one-step gradient descent update with four sub-steps:</span>
<span class="sd">        1) data loading, 2) cost calculation, 3) gradient calculation, and 4) parameter update.</span>
<span class="sd">        It also calls upper-level problems&#39; step methods after unrolling gradient steps based on</span>
<span class="sd">        the hierarchical dependency graph.</span>

<span class="sd">        :param global_step: global step of the whole multilevel optimization. Defaults to None.</span>
<span class="sd">        :type global_step: int, optional</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_global_step</span> <span class="o">=</span> <span class="n">global_step</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">step_normal</span><span class="p">(</span><span class="n">global_step</span><span class="o">=</span><span class="n">global_step</span><span class="p">)</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_count</span> <span class="o">%</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_unroll_steps</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">gas</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span>
            <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_count</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">warmup_steps</span>
        <span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">step_after_roll_back</span><span class="p">()</span></div>

<div class="viewcode-block" id="Problem.get_batch"><a class="viewcode-back" href="../../../betty/betty.problems.html#betty.problems.problem.Problem.get_batch">[docs]</a>    <span class="k">def</span> <span class="nf">get_batch</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Load training batch from the user-provided data loader</span>

<span class="sd">        :return: New training batch</span>
<span class="sd">        :rtype: Any</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">batch</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">get_batch_single_loader</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">train_data_loader</span><span class="p">))</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="n">batch</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span> <span class="k">else</span> <span class="n">batch</span></div>

<div class="viewcode-block" id="Problem.get_batch_single_loader"><a class="viewcode-back" href="../../../betty/betty.problems.html#betty.problems.problem.Problem.get_batch_single_loader">[docs]</a>    <span class="k">def</span> <span class="nf">get_batch_single_loader</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Load training batch from one of the user-provided data loader(s)</span>

<span class="sd">        :return: New training batch</span>
<span class="sd">        :rtype: Any</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">data_iterator</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_data_iterator</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">batch</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">data_iterator</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">StopIteration</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">idx</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">epoch_callback_exec</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">epoch_counter</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="n">train_data_loader</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_data_loader</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_strategy</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;distributed&quot;</span><span class="p">,</span> <span class="s2">&quot;zero&quot;</span><span class="p">,</span> <span class="s2">&quot;fsdp&quot;</span><span class="p">]:</span>
                <span class="n">train_data_loader</span><span class="o">.</span><span class="n">set_epoch</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">epoch_counter</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">train_data_iterator</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="nb">iter</span><span class="p">(</span><span class="n">train_data_loader</span><span class="p">)</span>
            <span class="n">batch</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">train_data_iterator</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
            <span class="n">batch</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span>
                <span class="n">convert_tensor</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_default_fp16</span><span class="p">())</span>
                <span class="k">for</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">batch</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">batch</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="n">batch</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">convert_tensor</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_default_fp16</span><span class="p">())</span>

        <span class="k">return</span> <span class="n">batch</span></div>

<div class="viewcode-block" id="Problem.get_loss"><a class="viewcode-back" href="../../../betty/betty.problems.html#betty.problems.problem.Problem.get_loss">[docs]</a>    <span class="k">def</span> <span class="nf">get_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Calculate loss and log metrics for the current batch based on the user-defined loss</span>
<span class="sd">        function.</span>

<span class="sd">        :return: loss and log metrics (e.g. classification accuracy)</span>
<span class="sd">        :rtype: dict</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">maybe_loss_dict</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_step_exec</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
        <span class="n">is_dict</span> <span class="o">=</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">maybe_loss_dict</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">maybe_loss_dict</span><span class="p">[</span><span class="s2">&quot;loss&quot;</span><span class="p">]</span> <span class="k">if</span> <span class="n">is_dict</span> <span class="k">else</span> <span class="n">maybe_loss_dict</span>
        <span class="n">loss_no_scale</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_default_fp16</span><span class="p">():</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">scaler</span><span class="o">.</span><span class="n">scale</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">gas</span>

        <span class="c1"># construct loss dict</span>
        <span class="n">loss_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;loss&quot;</span><span class="p">:</span> <span class="n">loss_no_scale</span><span class="p">}</span>
        <span class="k">if</span> <span class="n">is_dict</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">maybe_loss_dict</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="k">if</span> <span class="n">key</span> <span class="o">!=</span> <span class="s2">&quot;loss&quot;</span><span class="p">:</span>
                    <span class="n">loss_dict</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span>

        <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">loss_dict</span></div>

<div class="viewcode-block" id="Problem.backward"><a class="viewcode-back" href="../../../betty/betty.problems.html#betty.problems.problem.Problem.backward">[docs]</a>    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">loss</span><span class="p">,</span>
        <span class="n">params</span><span class="p">,</span>
        <span class="n">paths</span><span class="p">,</span>
        <span class="n">create_graph</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">retain_graph</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">allow_unused</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Calculate the gradient of ``loss`` with respect to ``params`` based on a user-defined</span>
<span class="sd">        ``config``.</span>

<span class="sd">        :param loss: Outputs of the differentiated function.</span>
<span class="sd">        :type loss: Tensor</span>
<span class="sd">        :param params: Inputs with respect to which the gradient will be returned.</span>
<span class="sd">        :type params: Sequence of Tensor</span>
<span class="sd">        :param paths: Paths on which the gradient will be calculated.</span>
<span class="sd">        :type paths: List of list of Problem</span>
<span class="sd">        :param create_graph:</span>
<span class="sd">            If ``True``, graph of the derivative will be constructed, allowing to compute higher order</span>
<span class="sd">            derivative products. Default: ``True``.</span>
<span class="sd">        :type create_graph: bool, optional</span>
<span class="sd">        :param retain_graph:</span>
<span class="sd">            If ``False``, the graph used to compute the grad will be freed. Note that in nearly all</span>
<span class="sd">            cases setting this option to ``True`` is not needed and often can be worked around in a much</span>
<span class="sd">            more efficient way. Defaults to the value of ``create_graph``.</span>
<span class="sd">        :type retain_graph: bool, optional</span>
<span class="sd">        :param allow_unused:</span>
<span class="sd">            If ``False``, specifying inputs that were not used when computing outputs (and therefore</span>
<span class="sd">            their grad is always zero) is an error. Defaults to ``False``.</span>
<span class="sd">        :type allow_unused: bool, optional</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># direct grad</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">paths</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">or</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">gradient_accumulation_boundary</span><span class="p">():</span>
            <span class="n">grads</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span>
                <span class="n">loss</span><span class="p">,</span>
                <span class="n">params</span><span class="p">,</span>
                <span class="n">create_graph</span><span class="o">=</span><span class="n">create_graph</span><span class="p">,</span>
                <span class="n">retain_graph</span><span class="o">=</span><span class="n">retain_graph</span><span class="p">,</span>
                <span class="n">allow_unused</span><span class="o">=</span><span class="n">allow_unused</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">set_grads</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">grads</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span>
                <span class="n">loss</span><span class="p">,</span>
                <span class="n">inputs</span><span class="o">=</span><span class="n">params</span><span class="p">,</span>
                <span class="n">create_graph</span><span class="o">=</span><span class="n">create_graph</span><span class="p">,</span>
                <span class="n">retain_graph</span><span class="o">=</span><span class="n">retain_graph</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="c1"># indirect grad: best-response Jacobian</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_config</span><span class="o">.</span><span class="n">first_order</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">path</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">paths</span><span class="p">):</span>
                <span class="n">retain_graph_implicit</span> <span class="o">=</span> <span class="kc">False</span> <span class="k">if</span> <span class="n">idx</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">paths</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span> <span class="k">else</span> <span class="kc">True</span>
                <span class="n">do_sync</span> <span class="o">=</span> <span class="nb">bool</span><span class="p">(</span>
                    <span class="n">idx</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">paths</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">gradient_accumulation_boundary</span><span class="p">()</span>
                <span class="p">)</span>
                <span class="n">grads</span> <span class="o">=</span> <span class="n">get_grads</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">path</span><span class="p">,</span> <span class="n">retain_graph_implicit</span><span class="p">,</span> <span class="n">do_sync</span><span class="p">)</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">do_sync</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">set_grads</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">grads</span><span class="p">)</span></div>

<div class="viewcode-block" id="Problem.set_grads"><a class="viewcode-back" href="../../../betty/betty.problems.html#betty.problems.problem.Problem.set_grads">[docs]</a>    <span class="k">def</span> <span class="nf">set_grads</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">grads</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Set gradients for trainable parameters. ``params.grad = grads``</span>

<span class="sd">        :param params: Trainable parameters</span>
<span class="sd">        :type params: Sequence of Tensor</span>
<span class="sd">        :param grads: Calculated gradient</span>
<span class="sd">        :type grads: Sequence of Tensor</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">param</span><span class="p">,</span> <span class="n">grad</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">grads</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="s2">&quot;grad&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">param</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">param</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">param</span><span class="o">.</span><span class="n">grad</span> <span class="o">+</span> <span class="n">grad</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">param</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">grad</span></div>

<div class="viewcode-block" id="Problem.synchronize_params"><a class="viewcode-back" href="../../../betty/betty.problems.html#betty.problems.problem.Problem.synchronize_params">[docs]</a>    <span class="k">def</span> <span class="nf">synchronize_params</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        synchronize parameters across distributed data-parallel processes</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_world_size</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_strategy</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;fsdp&quot;</span><span class="p">,</span> <span class="s2">&quot;accelerate&quot;</span><span class="p">]:</span>
            <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">params</span><span class="p">:</span>
                <span class="n">dist</span><span class="o">.</span><span class="n">broadcast</span><span class="p">(</span><span class="n">param</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span></div>

<div class="viewcode-block" id="Problem.optimizer_step"><a class="viewcode-back" href="../../../betty/betty.problems.html#betty.problems.problem.Problem.optimizer_step">[docs]</a>    <span class="nd">@abc</span><span class="o">.</span><span class="n">abstractmethod</span>
    <span class="k">def</span> <span class="nf">optimizer_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Update weights as in PyTorch&#39;s native ``optim.step()``</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span></div>

<div class="viewcode-block" id="Problem.zero_grad"><a class="viewcode-back" href="../../../betty/betty.problems.html#betty.problems.problem.Problem.zero_grad">[docs]</a>    <span class="k">def</span> <span class="nf">zero_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Set gradients for trainable parameters for the current problem to 0.</span>
<span class="sd">        Similar with PyTorch&#39;s ``optim.zero_grad()`` or ``module.zero_grad()``.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">trainable_parameters</span><span class="p">()):</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="s2">&quot;grad&quot;</span><span class="p">):</span>
                <span class="k">del</span> <span class="n">param</span><span class="o">.</span><span class="n">grad</span></div>

<div class="viewcode-block" id="Problem.clip_grad"><a class="viewcode-back" href="../../../betty/betty.problems.html#betty.problems.problem.Problem.clip_grad">[docs]</a>    <span class="k">def</span> <span class="nf">clip_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Perform gradient clipping based on the norm provided by Config</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_strategy</span> <span class="o">!=</span> <span class="s2">&quot;fsdp&quot;</span><span class="p">:</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">clip_grad_norm_</span><span class="p">(</span>
                <span class="n">parameters</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">trainable_parameters</span><span class="p">(),</span> <span class="n">max_norm</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">gradient_clipping</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">clip_grad_norm_</span><span class="p">(</span><span class="n">max_norm</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">gradient_clipping</span><span class="p">)</span></div>

<div class="viewcode-block" id="Problem.state_dict"><a class="viewcode-back" href="../../../betty/betty.problems.html#betty.problems.problem.Problem.state_dict">[docs]</a>    <span class="k">def</span> <span class="nf">state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return all states involved in ``Problem`` with a Python dictionary. By default, it</span>
<span class="sd">        includes ``self.module.state_dict`` and ``self.optimizer.state_dict``. Depending on users&#39;</span>
<span class="sd">        configurations, it may include ``self.scheuler.state_dict`` (lr scheduler) and</span>
<span class="sd">        ``self.scaler.state_dict`` (fp16 training)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">state_dict</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;module&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>
        <span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;optimizer&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">scheduler</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;scheduler&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">scheduler</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_default_fp16</span><span class="p">():</span>
            <span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;scaler&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">scaler</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">state_dict</span></div>

<div class="viewcode-block" id="Problem.load_state_dict"><a class="viewcode-back" href="../../../betty/betty.problems.html#betty.problems.problem.Problem.load_state_dict">[docs]</a>    <span class="k">def</span> <span class="nf">load_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_dict</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Load the state for the ``Problem``</span>

<span class="sd">        Args:</span>
<span class="sd">            state_dict (dict): Python dictionary of Problem states.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;module&quot;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;optimizer&quot;</span><span class="p">])</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">scheduler</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="s2">&quot;scheduler&quot;</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">scheduler</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;scheduler&quot;</span><span class="p">])</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_default_fp16</span><span class="p">()</span> <span class="ow">and</span> <span class="s2">&quot;scaler&quot;</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">scaler</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;scaler&quot;</span><span class="p">])</span></div>

<div class="viewcode-block" id="Problem.configure_distributed_training"><a class="viewcode-back" href="../../../betty/betty.problems.html#betty.problems.problem.Problem.configure_distributed_training">[docs]</a>    <span class="k">def</span> <span class="nf">configure_distributed_training</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dictionary</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Set the configuration for distributed training.</span>

<span class="sd">        :param dictionary: Python dictionary of distributed training provided by Engine.</span>
<span class="sd">        :type dictionary: dict</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_strategy</span> <span class="o">=</span> <span class="n">dictionary</span><span class="p">[</span><span class="s2">&quot;strategy&quot;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_backend</span> <span class="o">=</span> <span class="n">dictionary</span><span class="p">[</span><span class="s2">&quot;backend&quot;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_world_size</span> <span class="o">=</span> <span class="n">dictionary</span><span class="p">[</span><span class="s2">&quot;world_size&quot;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_rank</span> <span class="o">=</span> <span class="n">dictionary</span><span class="p">[</span><span class="s2">&quot;rank&quot;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_local_rank</span> <span class="o">=</span> <span class="n">dictionary</span><span class="p">[</span><span class="s2">&quot;local_rank&quot;</span><span class="p">]</span></div>

<div class="viewcode-block" id="Problem.configure_roll_back"><a class="viewcode-back" href="../../../betty/betty.problems.html#betty.problems.problem.Problem.configure_roll_back">[docs]</a>    <span class="k">def</span> <span class="nf">configure_roll_back</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">roll_back</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Set the roll-back (warm- start) option from Engine</span>

<span class="sd">        :param roll_back: roll-back (warm-start) on/off</span>
<span class="sd">        :type roll_back: bool</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_parents</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_roll_back</span> <span class="o">=</span> <span class="n">roll_back</span></div>

<div class="viewcode-block" id="Problem.configure_device"><a class="viewcode-back" href="../../../betty/betty.problems.html#betty.problems.problem.Problem.configure_device">[docs]</a>    <span class="k">def</span> <span class="nf">configure_device</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">device</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Set the device for the current problem.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">device</span></div>

<div class="viewcode-block" id="Problem.get_opt_param_group_for_param"><a class="viewcode-back" href="../../../betty/betty.problems.html#betty.problems.problem.Problem.get_opt_param_group_for_param">[docs]</a>    <span class="k">def</span> <span class="nf">get_opt_param_group_for_param</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">param</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Get optimizer param_group for specific parameter</span>

<span class="sd">        :param param: Parameter for which optimizer param_group is inquired</span>
<span class="sd">        :type param: torch.nn.Parameter</span>
<span class="sd">        :return: param_group for the given parameter</span>
<span class="sd">        :rtype: dict</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">param_groups</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span>
        <span class="k">for</span> <span class="n">group</span> <span class="ow">in</span> <span class="n">param_groups</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">group</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">]:</span>
                <span class="k">if</span> <span class="n">param</span> <span class="ow">is</span> <span class="n">p</span><span class="p">:</span>
                    <span class="k">return</span> <span class="n">group</span></div>

<div class="viewcode-block" id="Problem.get_opt_state_for_param"><a class="viewcode-back" href="../../../betty/betty.problems.html#betty.problems.problem.Problem.get_opt_state_for_param">[docs]</a>    <span class="k">def</span> <span class="nf">get_opt_state_for_param</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">param</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Get optimizer state for specific parameter</span>

<span class="sd">        :param param: Parameter for which optimizer state is inquired</span>
<span class="sd">        :type param: torch.nn.Parameter</span>
<span class="sd">        :return: optimizer state for the given parameter</span>
<span class="sd">        :rtype: dict</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">state</span>
        <span class="k">return</span> <span class="n">state</span><span class="p">[</span><span class="n">param</span><span class="p">]</span></div>

<div class="viewcode-block" id="Problem.cache_states"><a class="viewcode-back" href="../../../betty/betty.problems.html#betty.problems.problem.Problem.cache_states">[docs]</a>    <span class="nd">@abc</span><span class="o">.</span><span class="n">abstractmethod</span>
    <span class="k">def</span> <span class="nf">cache_states</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Cache params, buffers, optimizer states when ``config.roll_back`` is set to ``True`` in</span>
<span class="sd">        ``step``.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span></div>

<div class="viewcode-block" id="Problem.recover_states"><a class="viewcode-back" href="../../../betty/betty.problems.html#betty.problems.problem.Problem.recover_states">[docs]</a>    <span class="nd">@abc</span><span class="o">.</span><span class="n">abstractmethod</span>
    <span class="k">def</span> <span class="nf">recover_states</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Recover params, buffers, optimizer states when ``config.roll_back`` is set to ``True`` in</span>
<span class="sd">        ``step``.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span></div>

<div class="viewcode-block" id="Problem.epoch_callback_exec"><a class="viewcode-back" href="../../../betty/betty.problems.html#betty.problems.problem.Problem.epoch_callback_exec">[docs]</a>    <span class="k">def</span> <span class="nf">epoch_callback_exec</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_implemented</span><span class="p">(</span><span class="s2">&quot;epoch_callback&quot;</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">epoch_callback</span><span class="p">()</span></div>

<div class="viewcode-block" id="Problem.gradient_accumulation_boundary"><a class="viewcode-back" href="../../../betty/betty.problems.html#betty.problems.problem.Problem.gradient_accumulation_boundary">[docs]</a>    <span class="k">def</span> <span class="nf">gradient_accumulation_boundary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Check whether the current step is on the gradient accumulation boundary</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">bool</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_count</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">gas</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span></div>

    <span class="k">def</span> <span class="nf">_is_default_fp16</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Check whether to use PyTorch native fp16 (mixed-precision) feature</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fp16</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_strategy</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;accelerate&quot;</span><span class="p">]:</span>
            <span class="k">return</span> <span class="kc">False</span>
        <span class="k">return</span> <span class="kc">True</span>

<div class="viewcode-block" id="Problem.is_implemented"><a class="viewcode-back" href="../../../betty/betty.problems.html#betty.problems.problem.Problem.is_implemented">[docs]</a>    <span class="k">def</span> <span class="nf">is_implemented</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">fn_name</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Check if ``fn_name`` method is implemented in the class</span>

<span class="sd">        :rtype: bool</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">callable</span><span class="p">(</span><span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">fn_name</span><span class="p">,</span> <span class="kc">None</span><span class="p">))</span></div>

<div class="viewcode-block" id="Problem.check_ready"><a class="viewcode-back" href="../../../betty/betty.problems.html#betty.problems.problem.Problem.check_ready">[docs]</a>    <span class="k">def</span> <span class="nf">check_ready</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Check if unrolling processes of lower level problems in the hierarchical dependency</span>
<span class="sd">        graph are all ready/done. ``step`` function is only excuted when this method returns</span>
<span class="sd">        ``True``.</span>

<span class="sd">        :rtype: bool</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">all</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ready</span><span class="p">)</span></div>

<div class="viewcode-block" id="Problem.log"><a class="viewcode-back" href="../../../betty/betty.problems.html#betty.problems.problem.Problem.log">[docs]</a>    <span class="k">def</span> <span class="nf">log</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">stats</span><span class="p">,</span> <span class="n">global_step</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Log (training) stats to the ``self.logger``</span>

<span class="sd">        :param stats: log metrics such as loss and classification accuracy.</span>
<span class="sd">        :type stats: Any</span>
<span class="sd">        :param step: global/local step associated with the ``stats``.</span>
<span class="sd">        :type step: int</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">loss_log</span> <span class="o">=</span> <span class="n">log_from_loss_dict</span><span class="p">(</span><span class="n">stats</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">global_step</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                <span class="sa">f</span><span class="s1">&#39;[Problem &quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_name</span><span class="si">}</span><span class="s1">&quot;] [Local Step </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_count</span><span class="si">}</span><span class="s1">] </span><span class="si">{</span><span class="n">loss_log</span><span class="si">}</span><span class="s1">&#39;</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                <span class="sa">f</span><span class="s1">&#39;[Problem &quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_name</span><span class="si">}</span><span class="s1">&quot;] [Global Step </span><span class="si">{</span><span class="n">global_step</span><span class="si">}</span><span class="s1">] [Local Step </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_count</span><span class="si">}</span><span class="s1">] &#39;</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">loss_log</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>
        <span class="n">cur_step</span> <span class="o">=</span> <span class="n">global_step</span>
        <span class="k">if</span> <span class="n">global_step</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">log_local_step</span><span class="p">:</span>
            <span class="n">cur_step</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_count</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">stats</span><span class="p">,</span> <span class="n">tag</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_name</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="n">cur_step</span><span class="p">)</span></div>

<div class="viewcode-block" id="Problem.add_child"><a class="viewcode-back" href="../../../betty/betty.problems.html#betty.problems.problem.Problem.add_child">[docs]</a>    <span class="k">def</span> <span class="nf">add_child</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">problem</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Add ``problem`` to the lower-level problem list.</span>

<span class="sd">        :param problem: lower-level problem in the dependency graph</span>
<span class="sd">        :type problem: Problem</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">assert</span> <span class="n">problem</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_children</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_children</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">problem</span><span class="p">)</span></div>

<div class="viewcode-block" id="Problem.add_parent"><a class="viewcode-back" href="../../../betty/betty.problems.html#betty.problems.problem.Problem.add_parent">[docs]</a>    <span class="k">def</span> <span class="nf">add_parent</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">problem</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Add ``problem`` to the upper-level problem list.</span>

<span class="sd">        :param problem: upper-level problem in the dependency graph</span>
<span class="sd">        :type problem: Problem</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">assert</span> <span class="n">problem</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_parents</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_parents</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">problem</span><span class="p">)</span></div>

<div class="viewcode-block" id="Problem.add_paths"><a class="viewcode-back" href="../../../betty/betty.problems.html#betty.problems.problem.Problem.add_paths">[docs]</a>    <span class="k">def</span> <span class="nf">add_paths</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">paths</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Add new hypergradient backpropagation paths.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_paths</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">paths</span><span class="p">)</span></div>

<div class="viewcode-block" id="Problem.add_logger"><a class="viewcode-back" href="../../../betty/betty.problems.html#betty.problems.problem.Problem.add_logger">[docs]</a>    <span class="k">def</span> <span class="nf">add_logger</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">logger</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Add logger to the current problem.</span>

<span class="sd">        :param logger: logger defined by users in ``Engine``.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">logger</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">logger</span> <span class="o">=</span> <span class="n">logger</span></div>

<div class="viewcode-block" id="Problem.add_env"><a class="viewcode-back" href="../../../betty/betty.problems.html#betty.problems.problem.Problem.add_env">[docs]</a>    <span class="k">def</span> <span class="nf">add_env</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">env</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Add environment to the current problem.</span>

<span class="sd">        :param env: Environment.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">env</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">env</span> <span class="o">=</span> <span class="n">env</span></div>

<div class="viewcode-block" id="Problem.parameters"><a class="viewcode-back" href="../../../betty/betty.problems.html#betty.problems.problem.Problem.parameters">[docs]</a>    <span class="nd">@abc</span><span class="o">.</span><span class="n">abstractmethod</span>
    <span class="k">def</span> <span class="nf">parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return all parameters for the current problem.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span></div>

<div class="viewcode-block" id="Problem.trainable_parameters"><a class="viewcode-back" href="../../../betty/betty.problems.html#betty.problems.problem.Problem.trainable_parameters">[docs]</a>    <span class="nd">@abc</span><span class="o">.</span><span class="n">abstractmethod</span>
    <span class="k">def</span> <span class="nf">trainable_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Define all *trainable* parameters for the current problem.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span></div>

<div class="viewcode-block" id="Problem.clear_dependencies"><a class="viewcode-back" href="../../../betty/betty.problems.html#betty.problems.problem.Problem.clear_dependencies">[docs]</a>    <span class="k">def</span> <span class="nf">clear_dependencies</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Clear the dependencies of the current problem.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_children</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_parents</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_paths</span> <span class="o">=</span> <span class="p">[]</span></div>

<div class="viewcode-block" id="Problem.train"><a class="viewcode-back" href="../../../betty/betty.problems.html#betty.problems.problem.Problem.train">[docs]</a>    <span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Set the current problem to the training mode.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_training</span> <span class="o">=</span> <span class="kc">True</span></div>

<div class="viewcode-block" id="Problem.eval"><a class="viewcode-back" href="../../../betty/betty.problems.html#betty.problems.problem.Problem.eval">[docs]</a>    <span class="k">def</span> <span class="nf">eval</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Set the current problem to the evaluation mode.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_training</span> <span class="o">=</span> <span class="kc">False</span></div>

<div class="viewcode-block" id="Problem.is_rank_zero"><a class="viewcode-back" href="../../../betty/betty.problems.html#betty.problems.problem.Problem.is_rank_zero">[docs]</a>    <span class="k">def</span> <span class="nf">is_rank_zero</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Check whether the current device is rank 0.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_rank</span> <span class="o">==</span> <span class="mi">0</span></div>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">name</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;[summary]</span>
<span class="sd">        Return the user-defined name of the module.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_name</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">config</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return the configuration for the current problem.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_config</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">children</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return lower-level problems for the current problem.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_children</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">parents</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return upper-level problems for the current problem.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_parents</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">paths</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return hypergradient calculation paths for the current problem.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_paths</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">leaf</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return whether the current problem is leaf or not.</span>

<span class="sd">        :return: leaf</span>
<span class="sd">        :rtype: bool</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_leaf</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">count</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return the local step for the current problem.</span>

<span class="sd">        :return: local step</span>
<span class="sd">        :rtype: int</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_count</span>

    <span class="nd">@leaf</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">leaf</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">leaf</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Set the current problem as a leaf problem.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_leaf</span> <span class="o">=</span> <span class="n">leaf</span></div>
</pre></div>

             </article>
             
            </div>
            <footer>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2022, sangkeun00.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              
            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
         <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
         <script src="../../../_static/doctools.js"></script>
         <script src="../../../_static/sphinx_highlight.js"></script>
     

  

  <script type="text/javascript" src="../../../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../../../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
      </div>
    </div>
  </div>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. Read PyTorch Lightning's <a href="https://pytorchlightning.ai/privacy-policy">Privacy Policy</a>.</p>
    <img class="close-button" src="../../../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://leopard-ai.github.io/betty" aria-label="PyTorch Lightning"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://leopard-ai.github.io/betty">Docs</a>
          </li>

          <li>
            <a href="https://github.com/leopard-ai/betty">Github</a>
          </li>

        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../../../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>

  <!-- Google Tag Manager (noscript) -->
  <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-PQBQ3CV"
  height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
  <!-- End Google Tag Manager (noscript) -->
 </body>
</html>